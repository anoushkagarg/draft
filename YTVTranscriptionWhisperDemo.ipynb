{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anoushkagarg/draft/blob/main/YTVTranscriptionWhisperDemo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytube"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MRB4prIxFQvD",
        "outputId": "6fd1e1a1-d1b5-4af7-b4d8-618cd3a29549"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytube\n",
            "  Downloading pytube-15.0.0-py3-none-any.whl (57 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytube\n",
            "Successfully installed pytube-15.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1THkEoXiFGqC"
      },
      "outputs": [],
      "source": [
        "from pytube import YouTube"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QxVAiYThFGqE"
      },
      "outputs": [],
      "source": [
        "link='https://www.youtube.com/watch?v=dT_aIJZ7Mxw'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eCWYIW1ZaLvk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cgzjYcmBFGqE"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    # object creation using YouTube\n",
        "    # which was imported in the beginning\n",
        "    yt = YouTube(link)\n",
        "except:\n",
        "    print(\"Connection Error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ibA2bGLuFGqF",
        "outputId": "16e6e5df-810d-41e7-8fb6-d5ec15b5e43e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Stream: itag=\"18\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.42001E\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">, <Stream: itag=\"22\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.64001F\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">, <Stream: itag=\"137\" mime_type=\"video/mp4\" res=\"1080p\" fps=\"30fps\" vcodec=\"avc1.640028\" progressive=\"False\" type=\"video\">, <Stream: itag=\"136\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.64001f\" progressive=\"False\" type=\"video\">, <Stream: itag=\"135\" mime_type=\"video/mp4\" res=\"480p\" fps=\"30fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\">, <Stream: itag=\"134\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.4d401e\" progressive=\"False\" type=\"video\">, <Stream: itag=\"133\" mime_type=\"video/mp4\" res=\"240p\" fps=\"30fps\" vcodec=\"avc1.4d4015\" progressive=\"False\" type=\"video\">, <Stream: itag=\"160\" mime_type=\"video/mp4\" res=\"144p\" fps=\"30fps\" vcodec=\"avc1.4d400c\" progressive=\"False\" type=\"video\">, <Stream: itag=\"139\" mime_type=\"audio/mp4\" abr=\"48kbps\" acodec=\"mp4a.40.5\" progressive=\"False\" type=\"audio\">, <Stream: itag=\"140\" mime_type=\"audio/mp4\" abr=\"128kbps\" acodec=\"mp4a.40.2\" progressive=\"False\" type=\"audio\">]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        " yt.streams.filter(file_extension='mp4')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cQXglubeaQeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_mj3iS5eFGqF"
      },
      "outputs": [],
      "source": [
        "stream = yt.streams.get_by_itag(139)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EBbeebEZFGqG",
        "outputId": "1bc90914-cb69-4c1f-d6f4-a5d9d9eefc13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/GoogleImagen.mp4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\n",
        "stream.download('',\"GoogleImagen.mp4\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install jiwer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZPXSgQWQJwX0",
        "outputId": "6289d2f2-e14c-4292-80eb-078b07a2eed4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-m2kukgyz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-m2kukgyz\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.2)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117)\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802825 sha256=6cd25220125d5b3f229c36f81ce59715d8989e0747bf83b4fceab317065c605e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6zpixog4/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20231117 tiktoken-0.6.0\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.3 rapidfuzz-3.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "model = whisper.load_model(\"base\")\n",
        "result = model.transcribe(\"GoogleImagen.mp4\")\n",
        "print(result['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qJwhMsOhJgQm",
        "outputId": "54e5e1b5-cbfb-459c-b82c-4e7eaff72e02"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 183MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " In this video, let's look at image and a new AI system from Google Brain. So what does image and image and text as an input and then generates images. For example, the text over here is a chrome plated duck with a golden beak arguing with an angry turtle in a forest. So this is the text given and this is the image generated. So how does this system work? So this system takes text as an input. There is a frozen text encoder which produces a text embedding. This text embedding is then fed into a series of image diffusion models. There is a first text to image diffusion model which generates an image. Then there is a super resolution diffusion model which is used for increasing the resolution. Then you have another super resolution diffusion model which gives you the final image. And each of these diffusion models takes the text embedding as the input along with the previous output of the previous stage. So for the text encoder over here, they use a T5-XXL encoder to encode the input text into embeddings. Then they have conditional diffusion model maps that text embedding into the various 64 x 64 image. Then there is a further super text condition, super resolution diffusion model which can up sample the image. 64 image into the final image. And if you look at the photos over here which have been generated based on the captions, the photo quality and the photo realism is much high when compared to Dali. So here is another example which they show over here. So you can click on a word and you can see what is the kind of image which has been generated. For example, a photo of a British short hair cat wearing a cowboy hat and a red shirt riding a bike on a beach. So this is the kind of image you get. Now if you go to the other one, this is an oil painting of the same thing. For example, this is a Persian cat. This is the image which has been generated. So if you click on Fasi Penta. For the panda, this is the kind of image which has been generated based on this highlighted text caption. So this is playing a guitar, this is riding a bike, this is skate boarding on top of a mountain. So this is an oil painting. Now if you go to photo, you see this photo quality which has been generated. So this seems much better than Dali and they say that when they looked at the benchmarks over here, they have attained a new state of art score image fidelity score on when compared to Dali to or the other models on the cocoa dataset. So this is quite interesting work and I am amazed at the pace at which AI is developing now with newer models being developed by these big, big research teams achieving newer, fascinating results. I hope you liked this video on image and if you liked the video, please like share subscribe to the channel. See you in another video. Happy learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: the above output should be downloaded as srt file\n",
        "!pip install srt\n",
        "import srt\n",
        "!pip install datetime\n",
        "import datetime\n",
        "\n",
        "subtitles = []\n",
        "start_time = datetime.timedelta()\n",
        "for i, line in enumerate(result[\"segments\"]):\n",
        "  if \"duration\" in line:\n",
        "    end_time = start_time + datetime.timedelta(seconds=line[\"duration\"])\n",
        "  else:\n",
        "    # Handle the case where the \"duration\" key is missing\n",
        "    # For example, you could set the end time to a default value\n",
        "    end_time = start_time + datetime.timedelta(seconds=1)\n",
        "  subtitles.append(\n",
        "      srt.Subtitle(\n",
        "          index=i,\n",
        "          start=start_time,\n",
        "          end=end_time,\n",
        "          content=line[\"text\"],\n",
        "      )\n",
        "  )\n",
        "  start_time = end_time\n",
        "\n",
        "with open(\"output.srt\", \"w\") as f:\n",
        "  f.write(srt.compose(subtitles))\n",
        "\n",
        "!cat output.srt"
      ],
      "metadata": {
        "id": "EFtEStD6MGQZ",
        "outputId": "31dd7024-68fe-43eb-c524-822f9ed20061",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: srt in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Collecting datetime\n",
            "  Downloading DateTime-5.4-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zope.interface (from datetime)\n",
            "  Downloading zope.interface-6.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.3/247.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2023.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (67.7.2)\n",
            "Installing collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-5.4 zope.interface-6.2\n",
            "1\n",
            "00:00:00,000 --> 00:00:01,000\n",
            " In this video, let's look at image and a new AI system from Google Brain.\n",
            "\n",
            "2\n",
            "00:00:01,000 --> 00:00:02,000\n",
            " So what does image and image and text as an input and then generates images.\n",
            "\n",
            "3\n",
            "00:00:02,000 --> 00:00:03,000\n",
            " For example, the text over here is a chrome plated duck with a golden beak arguing with an angry turtle in a forest.\n",
            "\n",
            "4\n",
            "00:00:03,000 --> 00:00:04,000\n",
            " So this is the text given and this is the image generated.\n",
            "\n",
            "5\n",
            "00:00:04,000 --> 00:00:05,000\n",
            " So how does this system work?\n",
            "\n",
            "6\n",
            "00:00:05,000 --> 00:00:06,000\n",
            " So this system takes text as an input.\n",
            "\n",
            "7\n",
            "00:00:06,000 --> 00:00:07,000\n",
            " There is a frozen text encoder which produces a text embedding.\n",
            "\n",
            "8\n",
            "00:00:07,000 --> 00:00:08,000\n",
            " This text embedding is then fed into a series of image diffusion models.\n",
            "\n",
            "9\n",
            "00:00:08,000 --> 00:00:09,000\n",
            " There is a first text to image diffusion model which generates an image.\n",
            "\n",
            "10\n",
            "00:00:09,000 --> 00:00:10,000\n",
            " Then there is a super resolution diffusion model which is used for increasing the resolution.\n",
            "\n",
            "11\n",
            "00:00:10,000 --> 00:00:11,000\n",
            " Then you have another super resolution diffusion model which gives you the final image.\n",
            "\n",
            "12\n",
            "00:00:11,000 --> 00:00:12,000\n",
            " And each of these diffusion models takes the text embedding as the input along with the previous output of the previous stage.\n",
            "\n",
            "13\n",
            "00:00:12,000 --> 00:00:13,000\n",
            " So for the text encoder over here, they use a T5-XXL encoder to encode the input text into embeddings.\n",
            "\n",
            "14\n",
            "00:00:13,000 --> 00:00:14,000\n",
            " Then they have conditional diffusion model maps that text embedding into the various 64 x 64 image.\n",
            "\n",
            "15\n",
            "00:00:14,000 --> 00:00:15,000\n",
            " Then there is a further super text condition, super resolution diffusion model which can up sample the image.\n",
            "\n",
            "16\n",
            "00:00:15,000 --> 00:00:16,000\n",
            " 64 image into the final image.\n",
            "\n",
            "17\n",
            "00:00:16,000 --> 00:00:17,000\n",
            " And if you look at the photos over here which have been generated based on the captions,\n",
            "\n",
            "18\n",
            "00:00:17,000 --> 00:00:18,000\n",
            " the photo quality and the photo realism is much high when compared to Dali.\n",
            "\n",
            "19\n",
            "00:00:18,000 --> 00:00:19,000\n",
            " So here is another example which they show over here.\n",
            "\n",
            "20\n",
            "00:00:19,000 --> 00:00:20,000\n",
            " So you can click on a word and you can see what is the kind of image which has been generated.\n",
            "\n",
            "21\n",
            "00:00:20,000 --> 00:00:21,000\n",
            " For example, a photo of a British short hair cat wearing a cowboy hat and a red shirt riding a bike on a beach.\n",
            "\n",
            "22\n",
            "00:00:21,000 --> 00:00:22,000\n",
            " So this is the kind of image you get.\n",
            "\n",
            "23\n",
            "00:00:22,000 --> 00:00:23,000\n",
            " Now if you go to the other one, this is an oil painting of the same thing.\n",
            "\n",
            "24\n",
            "00:00:23,000 --> 00:00:24,000\n",
            " For example, this is a Persian cat.\n",
            "\n",
            "25\n",
            "00:00:24,000 --> 00:00:25,000\n",
            " This is the image which has been generated.\n",
            "\n",
            "26\n",
            "00:00:25,000 --> 00:00:26,000\n",
            " So if you click on Fasi Penta.\n",
            "\n",
            "27\n",
            "00:00:26,000 --> 00:00:27,000\n",
            " For the panda, this is the kind of image which has been generated based on this highlighted text caption.\n",
            "\n",
            "28\n",
            "00:00:27,000 --> 00:00:28,000\n",
            " So this is playing a guitar, this is riding a bike, this is skate boarding on top of a mountain.\n",
            "\n",
            "29\n",
            "00:00:28,000 --> 00:00:29,000\n",
            " So this is an oil painting.\n",
            "\n",
            "30\n",
            "00:00:29,000 --> 00:00:30,000\n",
            " Now if you go to photo, you see this photo quality which has been generated.\n",
            "\n",
            "31\n",
            "00:00:30,000 --> 00:00:31,000\n",
            " So this seems much better than Dali and they say that when they looked at the benchmarks over here,\n",
            "\n",
            "32\n",
            "00:00:31,000 --> 00:00:32,000\n",
            " they have attained a new state of art score image fidelity score on when compared to Dali to or the other models on the cocoa dataset.\n",
            "\n",
            "33\n",
            "00:00:32,000 --> 00:00:33,000\n",
            " So this is quite interesting work and I am amazed at the pace at which AI is developing now with newer models being developed by these big, big research teams achieving newer, fascinating results.\n",
            "\n",
            "34\n",
            "00:00:33,000 --> 00:00:34,000\n",
            " I hope you liked this video on image and if you liked the video, please like share subscribe to the channel.\n",
            "\n",
            "35\n",
            "00:00:34,000 --> 00:00:35,000\n",
            " See you in another video.\n",
            "\n",
            "36\n",
            "00:00:35,000 --> 00:00:36,000\n",
            " Happy learning.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dccL96N0ntJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "_h6zvcmyjMdE"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "source": [
        "# prompt: yes but dowloload this as srt file\n",
        "\n",
        "# Import the necessary libraries\n",
        "from pytube import YouTube\n",
        "import whisper\n",
        "import srt\n",
        "import datetime\n",
        "\n",
        "# Define the YouTube link\n",
        "link = 'https://www.youtube.com/watch?v=dT_aIJZ7Mxw'\n",
        "\n",
        "# Create a YouTube object\n",
        "yt = YouTube(link)\n",
        "\n",
        "# Filter the streams by file extension and get the desired stream\n",
        "stream = yt.streams.filter(file_extension='mp4').get_by_itag(139)\n",
        "\n",
        "# Download the video and save it as \"GoogleImagen.mp4\"\n",
        "stream.download('', \"GoogleImagen.mp4\")\n",
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Transcribe the audio from the video\n",
        "result = model.transcribe(\"GoogleImagen.mp4\")\n",
        "\n",
        "# Create a list to store the subtitles\n",
        "subtitles = []\n",
        "\n",
        "# Initialize the start time\n",
        "start_time = datetime.timedelta()\n",
        "\n",
        "# Iterate through the segments in the transcription result\n",
        "for i, line in enumerate(result[\"segments\"]):\n",
        "  if \"duration\" in line:\n",
        "    end_time = start_time + datetime.timedelta(seconds=line[\"duration\"])\n",
        "  else:\n",
        "    # Handle the case where the \"duration\" key is missing\n",
        "    # For example, you could set the end time to a default value\n",
        "    end_time = start_time + datetime.timedelta(seconds=1)\n",
        "  subtitles.append(\n",
        "      srt.Subtitle(\n",
        "          index=i,\n",
        "          start=start_time,\n",
        "          end=end_time,\n",
        "          content=line[\"text\"],\n",
        "      )\n",
        "  )\n",
        "  start_time = end_time\n",
        "\n",
        "# Write the subtitles to an SRT file\n",
        "with open(\"output.srt\", \"w\") as f:\n",
        "  f.write(srt.compose(subtitles))\n",
        "\n",
        "# Download the SRT file\n",
        "!cat output.srt\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9TFJ7HWpjtGs",
        "outputId": "d5a272b3-1235-4f03-c86f-f4c2d8c06119",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "00:00:00,000 --> 00:00:01,000\n",
            " In this video, let's look at image and a new AI system from Google Brain.\n",
            "\n",
            "2\n",
            "00:00:01,000 --> 00:00:02,000\n",
            " So what does image and image and text as an input and then generates images.\n",
            "\n",
            "3\n",
            "00:00:02,000 --> 00:00:03,000\n",
            " For example, the text over here is a chrome plated duck with a golden beak arguing with an angry turtle in a forest.\n",
            "\n",
            "4\n",
            "00:00:03,000 --> 00:00:04,000\n",
            " So this is the text given and this is the image generated.\n",
            "\n",
            "5\n",
            "00:00:04,000 --> 00:00:05,000\n",
            " So how does this system work?\n",
            "\n",
            "6\n",
            "00:00:05,000 --> 00:00:06,000\n",
            " So this system takes text as an input.\n",
            "\n",
            "7\n",
            "00:00:06,000 --> 00:00:07,000\n",
            " There is a frozen text encoder which produces a text embedding.\n",
            "\n",
            "8\n",
            "00:00:07,000 --> 00:00:08,000\n",
            " This text embedding is then fed into a series of image diffusion models.\n",
            "\n",
            "9\n",
            "00:00:08,000 --> 00:00:09,000\n",
            " There is a first text to image diffusion model which generates an image.\n",
            "\n",
            "10\n",
            "00:00:09,000 --> 00:00:10,000\n",
            " Then there is a super resolution diffusion model which is used for increasing the resolution.\n",
            "\n",
            "11\n",
            "00:00:10,000 --> 00:00:11,000\n",
            " Then you have another super resolution diffusion model which gives you the final image.\n",
            "\n",
            "12\n",
            "00:00:11,000 --> 00:00:12,000\n",
            " And each of these diffusion models takes the text embedding as the input along with the previous output of the previous stage.\n",
            "\n",
            "13\n",
            "00:00:12,000 --> 00:00:13,000\n",
            " So for the text encoder over here, they use a T5-XXL encoder to encode the input text into embeddings.\n",
            "\n",
            "14\n",
            "00:00:13,000 --> 00:00:14,000\n",
            " Then they have conditional diffusion model maps that text embedding into the various 64 x 64 image.\n",
            "\n",
            "15\n",
            "00:00:14,000 --> 00:00:15,000\n",
            " Then there is a further super text condition, super resolution diffusion model which can up sample the image.\n",
            "\n",
            "16\n",
            "00:00:15,000 --> 00:00:16,000\n",
            " 64 image into the final image.\n",
            "\n",
            "17\n",
            "00:00:16,000 --> 00:00:17,000\n",
            " And if you look at the photos over here which have been generated based on the captions,\n",
            "\n",
            "18\n",
            "00:00:17,000 --> 00:00:18,000\n",
            " the photo quality and the photo realism is much high when compared to Dali.\n",
            "\n",
            "19\n",
            "00:00:18,000 --> 00:00:19,000\n",
            " So here is another example which they show over here.\n",
            "\n",
            "20\n",
            "00:00:19,000 --> 00:00:20,000\n",
            " So you can click on a word and you can see what is the kind of image which has been generated.\n",
            "\n",
            "21\n",
            "00:00:20,000 --> 00:00:21,000\n",
            " For example, a photo of a British short hair cat wearing a cowboy hat and a red shirt riding a bike on a beach.\n",
            "\n",
            "22\n",
            "00:00:21,000 --> 00:00:22,000\n",
            " So this is the kind of image you get.\n",
            "\n",
            "23\n",
            "00:00:22,000 --> 00:00:23,000\n",
            " Now if you go to the other one, this is an oil painting of the same thing.\n",
            "\n",
            "24\n",
            "00:00:23,000 --> 00:00:24,000\n",
            " For example, this is a Persian cat.\n",
            "\n",
            "25\n",
            "00:00:24,000 --> 00:00:25,000\n",
            " This is the image which has been generated.\n",
            "\n",
            "26\n",
            "00:00:25,000 --> 00:00:26,000\n",
            " So if you click on Fasi Penta.\n",
            "\n",
            "27\n",
            "00:00:26,000 --> 00:00:27,000\n",
            " For the panda, this is the kind of image which has been generated based on this highlighted text caption.\n",
            "\n",
            "28\n",
            "00:00:27,000 --> 00:00:28,000\n",
            " So this is playing a guitar, this is riding a bike, this is skate boarding on top of a mountain.\n",
            "\n",
            "29\n",
            "00:00:28,000 --> 00:00:29,000\n",
            " So this is an oil painting.\n",
            "\n",
            "30\n",
            "00:00:29,000 --> 00:00:30,000\n",
            " Now if you go to photo, you see this photo quality which has been generated.\n",
            "\n",
            "31\n",
            "00:00:30,000 --> 00:00:31,000\n",
            " So this seems much better than Dali and they say that when they looked at the benchmarks over here,\n",
            "\n",
            "32\n",
            "00:00:31,000 --> 00:00:32,000\n",
            " they have attained a new state of art score image fidelity score on when compared to Dali to or the other models on the cocoa dataset.\n",
            "\n",
            "33\n",
            "00:00:32,000 --> 00:00:33,000\n",
            " So this is quite interesting work and I am amazed at the pace at which AI is developing now with newer models being developed by these big, big research teams achieving newer, fascinating results.\n",
            "\n",
            "34\n",
            "00:00:33,000 --> 00:00:34,000\n",
            " I hope you liked this video on image and if you liked the video, please like share subscribe to the channel.\n",
            "\n",
            "35\n",
            "00:00:34,000 --> 00:00:35,000\n",
            " See you in another video.\n",
            "\n",
            "36\n",
            "00:00:35,000 --> 00:00:36,000\n",
            " Happy learning.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "source": [
        "import datetime\n",
        "import srt\n",
        "from google.colab import files\n",
        "\n",
        "subtitles = []\n",
        "start_time = datetime.timedelta()\n",
        "\n",
        "for i, line in enumerate(result[\"segments\"]):\n",
        "    if \"duration\" in line:\n",
        "        end_time = start_time + datetime.timedelta(seconds=line[\"duration\"])\n",
        "    else:\n",
        "        # Handle the case where the \"duration\" key is missing\n",
        "        # For example, you could set the end time to a default value\n",
        "        end_time = start_time + datetime.timedelta(seconds=1)\n",
        "    subtitles.append(\n",
        "        srt.Subtitle(\n",
        "            index=i,\n",
        "            start=start_time,\n",
        "            end=end_time,\n",
        "            content=line[\"text\"],\n",
        "        )\n",
        "    )\n",
        "    start_time = end_time\n",
        "\n",
        "with open(\"output.srt\", \"w\") as f:\n",
        "    f.write(srt.compose(subtitles))\n",
        "\n",
        "# Download the SRT file\n",
        "files.download(\"output.srt\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "andIbLXMmek7",
        "outputId": "23eaab98-bd84-4aaf-ec10-e89aca706aec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3f61fb72-ac55-4e6e-82ee-bf88aa3951a5\", \"output.srt\", 4051)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import moviepy.editor as mpe\n",
        "\n",
        "def video_to_srt():\n",
        "  # Ask the user to upload a video\n",
        "  uploaded = files.upload()\n",
        "\n",
        "  # Get the filename of the uploaded video\n",
        "  filename = next(iter(uploaded))\n",
        "\n",
        "  # Convert the video to an SRT file\n",
        "  video = mpe.VideoFileClip(filename)\n",
        "  text = video.audio.write_audiofile(\"temp.wav\")\n",
        "  os.system(\"deepspeech --model deepspeech-0.9.3-models/deepspeech-0.9.3-models.pbmm --audio temp.wav -t output.srt\")\n",
        "\n",
        "  # Download the SRT file\n",
        "  files.download(\"output.srt\")\n",
        "\n",
        "  # Delete the temporary audio file\n",
        "  os.remove(\"temp.wav\")\n",
        "\n",
        "# Call the video_to_srt function\n",
        "video_to_srt()"
      ],
      "metadata": {
        "id": "5mpACMa9nuQc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}